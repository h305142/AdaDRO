# AdaDRO
AdaDRO: Adaptive Robust Classification Driven by Optimal Transport

 "Bootstrap Your Uncertainty: Adaptive Robust Classification Driven by Optimal-Transport" (submitted to NeurIPS 2025).

## Start

### Installation

```bash
pip install torch torchvision matplotlib seaborn scikit-learn numpy
```

### Basic Usage

```bash
python main.py --dataset cifar10 --epochs 100 --batch-size 128

python main.py --dataset colored_mnist --epochs 50 --batch-size 256

python main.py --dataset cifar100 --epochs 200 --batch-size 64
```

## ğŸ“ Project Structure

```
adadro_project/
â”œâ”€â”€ main.py                   
â”œâ”€â”€ config/                    
â”œâ”€â”€ models/                  
â”‚   â”œâ”€â”€ adadro_model.py        
â”‚   â”œâ”€â”€ moco.py               
â”‚   â””â”€â”€ backbone.py           
â”œâ”€â”€ losses/                    
â”‚   â”œâ”€â”€ adadro_loss.py        
â”‚   â””â”€â”€ optimal_transport.py 
â”œâ”€â”€ data/                     
â”œâ”€â”€ utils/                   
â”‚   â”œâ”€â”€ filtering.py         
â”‚   â”œâ”€â”€ mlmc.py              
â”‚   â””â”€â”€ metrics.py          
â””â”€â”€ training/                 
```

##  Key Features

### Two-Stage Training

1. **Semantic Calibration**: Learn semantic transport costs via inverse OT
   - Feature space IOT: MoCo InfoNCE loss
   - Label space IOT: Cross-entropy loss
2. **Adaptive DRO**: Robust optimization with dynamic uncertainty sets

### Core Components

- **Adaptive Filtering**: OT-driven reference distribution refinement
- **Semantic Transport Costs**: Cosine similarity-based feature/label costs
- **Worst-case Distribution**: Sinkhorn DRO with evolving uncertainty sets
- **MLMC Gradient Estimation**: Efficient gradient computation

##  Configuration

```bash
python main.py \
    --dataset cifar10 \           
    --arch resnet18 \           
    --epochs 200 \              
    --batch-size 256 \            
    --lr 0.01 \                  
    --lambda-reg 1.0 \           
    --kappa 1.0 \                
    --device cuda \              
    --experiment-name my_exp      
```

##  Algorithm Overview

### Core Mathematical Formulation

**Semantic Calibration (IOT Problems):**

```
min_Î¸ KL(Î³Ì„Ë£ | Î³áµ—Ë£), s.t. Î³áµ—Ë£ = argmin E[Cáµ—Ë£(xáµ¢,x'â±¼)] + ÎµH(Î³)
min_Î¸ KL(Î³Ì„ | Î³Î¸), s.t. Î³Î¸ = argmin E[CÎ¸(qáµ¢,k)] + ÎµH(Î³)
```

**Adaptive Filtering:**

```
Î½Ì‚(p) = Î½(p)Â·ğŸ™[âˆƒq: Î³Ì„Ë£(p,q)>0 âˆ§ Î³áµ—Ë£(p,q)â‰¥Ï„(q)] / Ï‡
```

**Worst-case Distribution:**

```
Qáµ(q) = Eâ‚š[exp((â„“(q)-Î»C(p,q))/(Î»Îµ)) / Z(p)] Â· Î½Ì‚(q)
```

### Theoretical Guarantees

- **Convergence**: O(Îµâ»â´logÂ²(1/Îµ)) sample complexity
- **Adaptivity**: Dynamic uncertainty set evolution
- **Robustness**: Performance guarantees under distribution shift

